{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow \n",
    "\n",
    "TensorFlow is a deep learning framework that allows you to build neural networks more easily than by hand, and thus can speed up your deep learning development significantly. However, TensorFlow is not just a deep learning library, but really a library for deep learning. It's really just a number-crunching library, similar to Numpy, but the difference is that TensorFlow allows us to perform machine-learning specific number-crunching operations (e.g. derivatives on huge matrices). Using TensorFlow, We can also easily distribute these processes across our CPU cores, GPU cores, but also across a distributed network of computers. \n",
    "\n",
    "If you are interested in installing TensorFlow in the future, we reccomend just going here for instructions: https://www.tensorflow.org/install/. We will be demonstrating TensorFlow in Python (obviously), but for those who are interested, they have APIs in the following languages: C++, Haskell, Java, Go, and Rust. TensorFlow is avaliable as third party packages in C#, Julia, R, and Scala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: A lot of this coding tutorial comes from Andrew Ng's Deep Learning course on Coursera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Alright, let's get started! First, let's import a couple of libraries we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Writing and running programs in TensorFlow has the following steps:\n",
    "\n",
    "1. Create Tensors (variables) that are not yet executed/evaluated. \n",
    "2. Write operations between those Tensors.\n",
    "3. Initialize your Tensors. \n",
    "4. Create a Session. \n",
    "5. Run the Session. This will run the operations you'd written above. \n",
    "\n",
    "Now let us look at an easy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2)\n",
    "b = tf.constant(10)\n",
    "c = tf.multiply(a,b)\n",
    "print(c) #Question: What should the output be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, you will not see 20! You got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type \"int32\". All you did was put in the 'computation graph', but you have not run this computation yet. In order to actually multiply the two numbers, you will have to create a session and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, **remember to initialize your variables, create a session and run the operations inside the session**. \n",
    "\n",
    "Quick aside: Note that there are two typical ways to create and use sessions in tensorflow: \n",
    "\n",
    "**Method 1:**\n",
    "```python\n",
    "sess = tf.Session()\n",
    "# Run the variables initialization (if needed), run the operations\n",
    "result = sess.run(..., feed_dict = {...})\n",
    "sess.close() # Close the session\n",
    "```\n",
    "**Method 2:**\n",
    "```python\n",
    "with tf.Session() as sess: \n",
    "    # run the variables initialization (if needed), run the operations\n",
    "    result = sess.run(..., feed_dict = {...})\n",
    "    # This takes care of closing the session for you :)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you'll also have to know about **placeholders**. A placeholder is an object whose value you can specify only later. \n",
    "To specify values for a placeholder, you can pass in values by using a \"feed dictionary\" (`feed_dict` variable). Let's see what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int64, name = 'x')\n",
    "print(sess.run(2 * x, feed_dict = {x: 3}))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we first defined `x` we did not have to specify a value for it. A placeholder is simply a variable that you will assign data to later when running the session. We say that you **feed data** to these placeholders when running the session. \n",
    "\n",
    "Here's what's happening: When you specify the operations needed for a computation, you are telling TensorFlow how to construct a computation graph. The computation graph can have some placeholders whose values you will specify only later. Finally, when you run the session, you are telling TensorFlow to execute the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Function in TensorFlow\n",
    "\n",
    "Let's start with a very simple exercise, by computing the following equation: $Y = WX + b$, where $W$ and $X$ are random matrices and $b$ is a random vector.\n",
    "\n",
    "Compute $WX + b$ where $W, X$, and $b$ are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result = [[-0.86056034]\n",
      " [ 0.99497496]\n",
      " [ 3.38770517]\n",
      " [ 2.7706892 ]]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(np.random.randn(3,1), name=\"X\")\n",
    "W = tf.constant(np.random.randn(4,3), name=\"W\")\n",
    "b = tf.constant(np.random.randn(4,1), name=\"b\")\n",
    "Y = tf.add(tf.matmul(W, X), b)\n",
    "\n",
    "sess = tf.Session()\n",
    "result = sess.run(Y)\n",
    "sess.close()\n",
    "print(\"Result = \" + str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid in TensorFlow\n",
    "\n",
    "So while we just saw that you can compute user defined functions, Tensorflow offers a variety of commonly used neural network functions like `tf.sigmoid` and `tf.softmax`. Let's compute the sigmoid function of an input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Computes the sigmoid of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- input value, scalar or vector\n",
    "    \n",
    "    Returns: \n",
    "    results -- the sigmoid of z\n",
    "    \"\"\"\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, name=\"x\")\n",
    "    sigmoid = tf.sigmoid(x)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        result = sess.run(sigmoid, feed_dict={x: z})\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0) = 0.5\n",
      "sigmoid(12) = 0.999994\n"
     ]
    }
   ],
   "source": [
    "print (\"sigmoid(0) = \" + str(sigmoid(0)))\n",
    "print (\"sigmoid(12) = \" + str(sigmoid(12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Cost\n",
    "\n",
    "You can also use a built-in function to compute the cost of your neural network. \n",
    "\n",
    "Let's implement the cross entropy loss. The function we will use is: \n",
    "\n",
    "- `tf.nn.sigmoid_cross_entropy_with_logits(logits = ...,  labels = ...)`\n",
    "\n",
    "We will input `z`, compute the sigmoid (to get `a`) and then compute the cross entropy cost $J$. All this can be done using one call to `tf.nn.sigmoid_cross_entropy_with_logits`, which computes\n",
    "\n",
    "$$- \\frac{1}{m}  \\sum_{i = 1}^m  \\large ( \\small y^{(i)} \\log \\sigma(z^{i}) + (1-y^{(i)})\\log (1-\\sigma(z^{i})\\large )\\small\\tag{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(logits, labels):\n",
    "    \"\"\"\n",
    "    Computes the cost using the sigmoid cross entropy\n",
    "    \n",
    "    Arguments:\n",
    "    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n",
    "    labels -- vector of labels y (1 or 0) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- runs the session of the cost (formula (2))\n",
    "    \"\"\"\n",
    "    z = tf.placeholder(tf.float32, name=\"z\")\n",
    "    y = tf.placeholder(tf.float32, name=\"y\")\n",
    "    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z, labels = y)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    cost = sess.run(cost, feed_dict={z: logits, y: labels})\n",
    "    sess.close()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = [ 1.00538719  1.03664076  0.41385433  0.39956617]\n"
     ]
    }
   ],
   "source": [
    "logits = sigmoid(np.array([0.2,0.4,0.7,0.9]))\n",
    "cost = cost(logits, np.array([0,0,1,1]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encodings\n",
    "\n",
    "Many times in deep learning you will have a y vector with numbers ranging from 0 to C-1, where C is the number of classes. If C is for example 4, then you might have to convert as follows:\n",
    "\n",
    "<img src=\"images/one_hot.png\">\n",
    "\n",
    "This is called a \"one hot\" encoding, because in the converted representation exactly one element of each column is \"hot\" (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In tensorflow, you can use one line of code: \n",
    "\n",
    "- tf.one_hot(labels, depth, axis) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    C = tf.constant(C, name=\"C\")\n",
    "    one_hot_matrix = tf.one_hot(labels, C, 1)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network with TensorFlow\n",
    "\n",
    "Now that we have seen a little bit about how TensorFlow works, let's build our first neural network. To begin, we will import the MNIST data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 55000\n",
      "Number of test examples = 10000\n",
      "X_train shape: (55000, 784)\n",
      "Y_train shape: (55000, 10)\n",
      "X_test shape: (10000, 784)\n",
      "Y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "Y_test = mnist.test.labels\n",
    "\n",
    "print (\"Number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"Number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what does this mean? In our data set, there are 55,000 examples of handwritten digits from zero to nine. Each example is a 28x28 pixel image flattened in an array with 784 values representing each pixel’s intensity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our goal** is to build an algorithm capable of recognizing a digit with high accuracy. To do so, we are going to build a tensorflow neural network model. \n",
    "\n",
    "**The model** is *LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX*.  \n",
    "\n",
    "### Create Placeholders\n",
    "\n",
    "Our first task is to create placeholders for `X` and `Y`. This will allow you to later pass your training data in when you run your session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(num_features, num_classes):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    num_features -- scalar, size of an image vector (num_px * num_px = 28 * 28 = 748)\n",
    "    num_classes -- scalar, number of classes (from 0 to 9, so -> 10)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: ** When we assign None to our placeholder, it means the placeholder can be fed as many examples as you want to give it. In this case, our placeholder can be fed any multitude of 784-sized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder_1:0\", shape=(?, 784), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_2:0\", shape=(?, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(X_train.shape[1], Y_train.shape[1])\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(num_features, num_classes):\n",
    "    \"\"\"\n",
    "    Initializes parameters to build a neural network with tensorflow. \n",
    "    \n",
    "    Arguments:\n",
    "    num_features -- scalar, size of an image vector (num_px * num_px = 28 * 28 = 748)\n",
    "    num_classes -- scalar, number of classes (from 0 to 9, so -> 10)\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n",
    "    \"\"\"\n",
    "        \n",
    "    W1 = tf.get_variable(\"W1\", [num_features, 25], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", [1,25], initializer = tf.zeros_initializer())\n",
    "    W2 =  tf.get_variable(\"W2\", [25, 12], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [1,12], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [12, num_classes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", [1, num_classes], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(784, 25) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(1, 25) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(25, 12) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(1, 12) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(X_train.shape[1], Y_train.shape[1])\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the parameters haven't been evaluated yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (Number of examples, number of features)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "                                                           # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(X, W1), b1)                      # Z1 = np.dot(X, W1) + b1\n",
    "    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(A1, W2), b2)                     # Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(A2, W3), b3)                     # Z3 = np.dot(A2, W3) + b3\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward propagation & parameter updates\n",
    "\n",
    "This is where you become grateful to programming frameworks. All the backpropagation and the parameters update is taken care of in 1 line of code. It is very easy to incorporate this line in the model.\n",
    "\n",
    "After you compute the cost function. You will create an \"`optimizer`\" object. You have to call this object along with the cost when running the tf.session. When called, it will perform an optimization on the given cost with the chosen method and learning rate.\n",
    "\n",
    "For instance, for gradient descent the optimizer would be:\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "```\n",
    "\n",
    "To make the optimization you would do:\n",
    "```python\n",
    "_ , c = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "```\n",
    "\n",
    "This computes the backpropagation by passing through the tensorflow graph in the reverse order. From cost to inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model\n",
    "\n",
    "Now, you will bring it all together! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data\n",
    "    Y -- true \"label\" vector\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 200, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set features\n",
    "    Y_train -- training set class values\n",
    "    X_test -- test set features\n",
    "    Y_test -- test set class values\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    (m, num_features) = X_train.shape                 # (m : number of examples in the train set, n_features: input size)\n",
    "    num_classes = Y_train.shape[1]                      # n_classes : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create placeholders\n",
    "    X, Y = create_placeholders(num_features, num_classes)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(num_features, num_classes)\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                           # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 10 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z3,1), tf.argmax(Y,1))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.440457\n",
      "Cost after epoch 10: 0.218371\n",
      "Cost after epoch 20: 0.165692\n",
      "Cost after epoch 30: 0.137534\n",
      "Cost after epoch 40: 0.118825\n",
      "Cost after epoch 50: 0.104022\n",
      "Cost after epoch 60: 0.092475\n",
      "Cost after epoch 70: 0.082840\n",
      "Cost after epoch 80: 0.075074\n",
      "Cost after epoch 90: 0.068317\n",
      "Cost after epoch 100: 0.062230\n",
      "Cost after epoch 110: 0.057163\n",
      "Cost after epoch 120: 0.052521\n",
      "Cost after epoch 130: 0.048357\n",
      "Cost after epoch 140: 0.044474\n",
      "Cost after epoch 150: 0.040862\n",
      "Cost after epoch 160: 0.037575\n",
      "Cost after epoch 170: 0.034576\n",
      "Cost after epoch 180: 0.031857\n",
      "Cost after epoch 190: 0.029038\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27098878470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.994309\n",
      "Test Accuracy: 0.9629\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**Major Takeaways**:\n",
    "- Tensorflow is a programming framework used in deep learning\n",
    "- The two main object classes in tensorflow are Tensors and Operators. \n",
    "- When you code in tensorflow you have to take the following steps:\n",
    "    - Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)\n",
    "    - Create a session\n",
    "    - Initialize the session\n",
    "    - Run the session to execute the graph\n",
    "- You can execute the graph multiple times as you've seen in model()\n",
    "- The backpropagation and optimization is automatically done when running the session on the \"optimizer\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
